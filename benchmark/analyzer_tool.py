from cassandra.cluster import Cluster
from datetime import datetime
import polars as pl
import os
import sys
import argparse

# C·∫•u h√¨nh Cassandra
HOSTS = ['localhost']
KEYSPACE = os.getenv("CASSANDRA_KEYSPACE", "air_quality")
TABLE = os.getenv("CASSANDRA_TABLE", "realtime_data")

def analyze(limit=5000, output_file=None):
    print(f"üîç [BENCHMARK ANALYZER] Fetching last {limit} records...")
    
    try:
        cluster = Cluster(HOSTS, port=9042)
        session = cluster.connect(KEYSPACE)
        # Ch·ªâ l·∫•y 2 c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ gi·∫£m t·∫£i m·∫°ng
        query = f"SELECT datetime, processed_at FROM {TABLE} LIMIT {limit}"
        rows = session.execute(query)
    except Exception as e:
        sys.exit(f"‚ùå L·ªói Cassandra: {e}")

    # 1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu th√¥
    data_list = []
    for row in rows:
        if row.datetime and row.processed_at:
            try:
                # row.datetime l√† String (t·ª´ Producer), row.processed_at l√† datetime object (t·ª´ Spark)
                # Parse string ISO sang datetime v√† b·ªè timezone ƒë·ªÉ tr·ª´
                event_time = datetime.fromisoformat(row.datetime).replace(tzinfo=None)
                proc_time = row.processed_at.replace(tzinfo=None)
                
                latency = (proc_time - event_time).total_seconds()
                
                # L·ªçc nhi·ªÖu: ch·ªâ l·∫•y gi√° tr·ªã d∆∞∆°ng v√† < 5 ph√∫t
                if 0 <= latency < 300:
                    data_list.append({
                        "event_time": event_time,
                        "latency": latency
                    })
            except Exception:
                continue
    
    cluster.shutdown()

    if not data_list:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá (ho·∫∑c d·ªØ li·ªáu qu√° c≈©/l·ªách gi·ªù).")
        return

    # 2. Chuy·ªÉn sang Polars DataFrame
    df = pl.DataFrame(data_list)
    
    # 3. T√≠nh to√°n th·ªëng k√™ b·∫±ng Polars Expressions
    stats = df.select([
        pl.col("latency").count().alias("count"),
        pl.col("latency").min().alias("min"),
        pl.col("latency").max().alias("max"),
        pl.col("latency").mean().alias("avg"),
        pl.col("latency").std().alias("std_dev"),   # ƒê·ªô l·ªách chu·∫©n (Jitter)
        pl.col("latency").median().alias("p50"),    # Trung v·ªã
        pl.col("latency").quantile(0.95).alias("p95"),
        pl.col("latency").quantile(0.99).alias("p99") # Tail Latency
    ])

    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    print("\n" + "="*50)
    print("üìä K·∫æT QU·∫¢ PH√ÇN T√çCH HI·ªÜU NƒÇNG (SYSTEM LATENCY)")
    print("="*50)
    res = stats.to_dicts()[0]
    
    print(f"üîπ M·∫´u th·ª≠ (Sample):    {res['count']} records")
    print(f"üîπ Min / Max:           {res['min']:.4f}s / {res['max']:.4f}s")
    print(f"üîπ Trung b√¨nh (Avg):    {res['avg']:.4f}s")
    print(f"üîπ Trung v·ªã (P50):      {res['p50']:.4f}s")
    print("-" * 25)
    print(f"üî∏ P95 (Tail Latency):  {res['p95']:.4f}s")
    print(f"üî∏ P99 (Critical Tail): {res['p99']:.4f}s")
    print(f"üî∏ ƒê·ªô ·ªïn ƒë·ªãnh (StdDev): {res['std_dev']:.4f}s")
    print("="*50)

    # 4. Xu·∫•t file CSV n·∫øu c·∫ßn
    if output_file:
        df.write_csv(output_file)
        print(f"üíæ ƒê√£ l∆∞u raw data v√†o: {output_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--limit", type=int, default=5000)
    parser.add_argument("--output", type=str, default=None)
    args = parser.parse_args()
    analyze(limit=args.limit, output_file=args.output)