from cassandra.cluster import Cluster
from datetime import datetime, timedelta
import polars as pl
import os
import sys
import argparse

# Cáº¥u hÃ¬nh Cassandra
HOSTS = ['localhost']
KEYSPACE = os.getenv("CASSANDRA_KEYSPACE", "air_quality")
TABLE = os.getenv("CASSANDRA_TABLE", "realtime_data")

def analyze(limit=5000, output_file=None, valid_window_minutes=60):
    """
    PhÃ¢n tÃ­ch Ä‘á»™ trá»… cá»§a há»‡ thá»‘ng.
    :param limit: Sá»‘ lÆ°á»£ng báº£n ghi tá»‘i Ä‘a láº¥y tá»« DB.
    :param output_file: ÄÆ°á»ng dáº«n file CSV Ä‘á»ƒ lÆ°u káº¿t quáº£ (náº¿u cÃ³).
    :param valid_window_minutes: Chá»‰ cháº¥p nháº­n dá»¯ liá»‡u trong khoáº£ng thá»i gian nÃ y (trÃ¡nh tÃ­nh toÃ¡n nháº§m dá»¯ liá»‡u cÅ©).
    """
    print(f"ğŸ” [BENCHMARK ANALYZER] Connecting to Cassandra...")
    print(f"   - Fetching last {limit} records.")
    print(f"   - Valid Time Window: Last {valid_window_minutes} minutes.")

    try:
        cluster = Cluster(HOSTS, port=9042)
        session = cluster.connect(KEYSPACE)
        # Láº¥y dá»¯ liá»‡u. LÆ°u Ã½: Cassandra LIMIT khÃ´ng Ä‘áº£m báº£o thá»© tá»± náº¿u khÃ´ng cÃ³ WHERE,
        # nÃªn ta sáº½ lá»c láº¡i báº±ng Python bÃªn dÆ°á»›i.
        query = f"SELECT datetime, processed_at FROM {TABLE} LIMIT {limit}"
        rows = session.execute(query)
    except Exception as e:
        sys.exit(f"âŒ Lá»—i káº¿t ná»‘i Cassandra: {e}")

    # 1. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (Client-side Filtering)
    data_list = []
    skipped_old = 0
    skipped_error = 0
    
    # XÃ¡c Ä‘á»‹nh má»‘c thá»i gian há»£p lá»‡ (Hiá»‡n táº¡i - Window)
    now = datetime.now()
    cutoff_time = now - timedelta(minutes=valid_window_minutes)

    for row in rows:
        if row.datetime and row.processed_at:
            try:
                # Parse event_time (tá»« Producer - String ISO)
                # LÆ°u Ã½: fromisoformat cÃ³ thá»ƒ cÃ³ timezone hoáº·c khÃ´ng
                event_time = datetime.fromisoformat(row.datetime)
                
                # Parse processed_at (tá»« Spark - datetime object)
                proc_time = row.processed_at

                # Quy chuáº©n vá» Native Datetime (bá» mÃºi giá») Ä‘á»ƒ so sÃ¡nh
                if event_time.tzinfo:
                    event_time = event_time.replace(tzinfo=None)
                if proc_time.tzinfo:
                    proc_time = proc_time.replace(tzinfo=None)

                # --- QUAN TRá»ŒNG: Lá»c dá»¯ liá»‡u cÅ© ---
                if event_time < cutoff_time:
                    skipped_old += 1
                    continue

                # TÃ­nh Ä‘á»™ trá»…
                latency = (proc_time - event_time).total_seconds()
                
                # Lá»c nhiá»…u: chá»‰ láº¥y giÃ¡ trá»‹ dÆ°Æ¡ng vÃ  < 5 phÃºt (trÃ¡nh clock skew quÃ¡ lá»›n)
                if 0 <= latency < 300:
                    data_list.append({
                        "event_time": event_time,
                        "processed_at": proc_time,
                        "latency": latency
                    })
                else:
                    skipped_error += 1
            except Exception:
                skipped_error += 1
                continue
    
    cluster.shutdown()

    # BÃ¡o cÃ¡o sÆ¡ bá»™ vá» dá»¯ liá»‡u
    print(f"   - Raw records fetched: {len(list(rows)) if 'rows' in locals() else 'Unknown'}")
    print(f"   - Valid records:       {len(data_list)}")
    print(f"   - Skipped (Old Data):  {skipped_old} (Out of window)")
    print(f"   - Skipped (Invalid):   {skipped_error} (Negative latency/Error)")

    if not data_list:
        print("\nâš ï¸  KHÃ”NG CÃ“ Dá»® LIá»†U Há»¢P Lá»† Äá»‚ PHÃ‚N TÃCH.")
        print("   -> Gá»£i Ã½: HÃ£y kiá»ƒm tra láº¡i Timezone hoáº·c Producer cÃ³ Ä‘ang cháº¡y khÃ´ng?")
        return

    # 2. Chuyá»ƒn sang Polars DataFrame
    df = pl.DataFrame(data_list)
    
    # 3. TÃ­nh toÃ¡n thá»‘ng kÃª
    stats = df.select([
        pl.col("latency").count().alias("count"),
        pl.col("latency").min().alias("min"),
        pl.col("latency").max().alias("max"),
        pl.col("latency").mean().alias("avg"),
        pl.col("latency").std().alias("std_dev"),
        pl.col("latency").median().alias("p50"),
        pl.col("latency").quantile(0.95).alias("p95"),
        pl.col("latency").quantile(0.99).alias("p99")
    ])

    # Hiá»ƒn thá»‹ káº¿t quáº£
    res = stats.to_dicts()[0]
    
    print("\n" + "="*50)
    print("ğŸ“Š Káº¾T QUáº¢ PHÃ‚N TÃCH HIá»†U NÄ‚NG (SYSTEM LATENCY)")
    print("="*50)
    print(f"ğŸ”¹ Máº«u thá»­ (Samples):   {res['count']}")
    print(f"ğŸ”¹ Min Latency:         {res['min']:.4f} s")
    print(f"ğŸ”¹ Max Latency:         {res['max']:.4f} s")
    print(f"ğŸ”¹ Trung bÃ¬nh (Avg):    {res['avg']:.4f} s")
    print("-" * 50)
    print(f"ğŸ”¸ P50 (Median):        {res['p50']:.4f} s")
    print(f"ğŸ”¸ P95 (Tail Latency):  {res['p95']:.4f} s")
    print(f"ğŸ”¸ P99 (Critical):      {res['p99']:.4f} s")
    print(f"ğŸ”¸ Jitter (StdDev):     {res['std_dev']:.4f} s")
    print("="*50)

    # Cáº£nh bÃ¡o hiá»‡u nÄƒng
    if res['avg'] > 1.0:
        print("âš ï¸  Cáº¢NH BÃO: Äá»™ trá»… trung bÃ¬nh > 1s. Há»‡ thá»‘ng cÃ³ thá»ƒ Ä‘ang quÃ¡ táº£i.")
    if res['std_dev'] > 0.5:
        print("âš ï¸  Cáº¢NH BÃO: Äá»™ á»•n Ä‘á»‹nh tháº¥p (Jitter cao).")

    # 4. Xuáº¥t file CSV náº¿u cáº§n
    if output_file:
        # Sort theo thá»i gian trÆ°á»›c khi lÆ°u Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ Ä‘áº¹p hÆ¡n
        df = df.sort("event_time")
        df.write_csv(output_file)
        print(f"\nğŸ’¾ ÄÃ£ lÆ°u raw data Ä‘Ã£ lá»c vÃ o: {output_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--limit", type=int, default=5000, help="Sá»‘ lÆ°á»£ng báº£n ghi tá»‘i Ä‘a load tá»« DB")
    parser.add_argument("--output", type=str, default=None, help="ÄÆ°á»ng dáº«n file output CSV")
    parser.add_argument("--window", type=int, default=60, help="Cá»­a sá»• thá»i gian há»£p lá»‡ (phÃºt) so vá»›i hiá»‡n táº¡i")
    
    args = parser.parse_args()
    analyze(limit=args.limit, output_file=args.output, valid_window_minutes=args.window)